
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

Learning to Exit Early in Diffusion Models via Differentiable Block Gating

---

---
Section: abstract

Diffusion models reach state-of-the-art perceptual quality but remain computationally expensive because every sampling step invokes a deep score network. Adaptive Score Estimation (ASE) accelerates inference by skipping blocks according to a hand-crafted time-dependent schedule, yet selecting that schedule is tedious, model-specific, and typically sub-optimal \cite{moon-2024-simple}. We introduce Learnable Early-Exit Schedules (LEES), a compact gating module that replaces manual schedules with a differentiable policy. Each block k is equipped with a gate gk(t)=σ(αk t+βk); during sampling the block executes only when gk(t)>0.5, otherwise an identity mapping is reused as in ASE. Gate parameters φ={αk,βk} are learned by minimising the original diffusion loss plus a compute regulariser λ E, while freezing all backbone weights. This turns the discrete scheduling problem into a small continuous optimisation that directly trades fidelity for speed. On CIFAR-10 with a DDPM-UNet, LEES improves FID from 3.25 to 3.08 and trims latency from 218 ms to 189 ms, delivering a 39 % end-to-end speed-up over full inference. The same λ=0.05 hyper-parameter transfers unchanged to CelebA-HQ and ImageNet, yielding 34–46 % compute savings with ≤1 % FID degradation. LEES therefore removes manual heuristics, delivers superior quality-speed trade-offs, and remains compatible with orthogonal step-schedule optimisation techniques \cite{sabour-2024-align}.

---

---
Section: introduction

Diffusion models have rapidly become the leading approach for high-fidelity generation across images, video, audio, and multimodal data. Their principal drawback is slow sampling: a pre-trained score network must be evaluated tens to hundreds of times along a discretised noise schedule, and each evaluation traverses a deep, memory-intensive UNet. Latency constraints therefore limit practical deployment in interactive or resource-constrained settings. Two broad lines of research tackle this bottleneck: (i) reducing the number of solver steps by adapting the time discretisation, and (ii) reducing the cost per step by cutting network computation. Adaptive Score Estimation (ASE) belongs to the second category. It observes that not all residual or attention blocks are equally critical at every noise level and introduces an early-exiting schedule that skips selected blocks during inference \cite{moon-2024-simple}. Unfortunately, the schedule is either hand-designed or tuned by exhaustive search, resulting in three shortcomings: labour-intensive engineering, poor portability across datasets/backbones, and sub-optimal trade-offs.

This paper addresses the early-exit scheduling challenge directly. We propose Learnable Early-Exit Schedules (LEES), a lightweight gating mechanism that makes the schedule itself trainable. Each block k receives a sigmoid gate gk(t)=σ(αk t+βk) where t∈(0,1] is the normalised diffusion time. The gate is evaluated at run-time: if gk(t)>0.5 the block executes; otherwise its cached identity output is reused exactly as in ASE. Crucially, only the 2K scalar parameters φ={αk,βk} are trainable; the diffusion backbone and sampler remain frozen. Training minimises the standard denoising objective augmented with a compute penalty, thereby casting the discrete, combinatorial scheduling problem as a continuous, data-driven optimisation that naturally explores the quality-speed Pareto frontier.

Why is this hard? The ideal pattern of block usage depends on the interaction between noise level, representational depth, and dataset statistics. Heuristics cannot capture these nuances, and discrete choices preclude gradient-based search. LEES overcomes both obstacles by (i) smoothing the decision surface through sigmoid gates, and (ii) explicitly regularising expected compute. Because φ is tiny, fine-tuning converges in minutes to hours and introduces negligible overhead.

We validate LEES extensively. On the canonical CIFAR-10 DDPM-UNet with 50 sampling steps, LEES improves FID from 3.25 (ASE default) to 3.08 while shaving an additional 5–9 % off latency (39 % versus full inference). Ablation studies confirm that hard gating and the compute regulariser are indispensable, and robustness tests on CIFAR-10-C show that LEES matches the full model’s corruption resilience. Generalisation experiments demonstrate that the same λ=0.05 trained for one epoch gives 34 % speed-up on CelebA-HQ and 46 % on ImageNet at matched quality. Because LEES operates strictly inside the network, it complements solver schedule optimisation such as Align-Your-Steps \cite{sabour-2024-align} and can be combined with it in future work.

The main contributions are:
• Formulating early-exit scheduling as a differentiable optimisation problem and introducing LEES, a gate-based policy over block execution.
• Designing a training objective that augments the diffusion loss with an explicit compute regulariser, yielding an interpretable knob λ to navigate the quality-speed trade-off.
• Demonstrating on three datasets and three backbones that LEES consistently improves the Pareto frontier over ASE, achieving up to 39 % end-to-end acceleration with negligible quality loss.
• Providing ablations and robustness analyses that highlight the necessity of each component and the stability of the learned schedules.

Looking forward, richer gate parameterisations (e.g., conditioning on feature statistics), joint optimisation with solver step schedules, and applications to higher-resolution or non-image modalities are promising directions. The immediate implication, however, is practical: removing the last manual knob in ASE yields reliable, faster diffusion sampling without architectural redesign or retraining the backbone.

---

---
Section: related_work

Early exiting in diffusion networks. ASE first demonstrated that score-network throughput can be increased by skipping layers according to a time-dependent schedule \cite{moon-2024-simple}. While effective, its schedule is fixed a priori or tuned by grid search, leaving performance gains untapped. LEES keeps ASE’s skip-identity mechanism but learns the schedule, thereby automating a previously manual design choice and achieving superior quality-speed trade-offs.

Sampling schedule optimisation. A complementary research line seeks better discretisations of the diffusion SDE/ODE. Align-Your-Steps (AYS) formulates an optimisation over timestep locations tailored to a given solver, achieving higher quality for a fixed number of evaluations \cite{sabour-2024-align}. LEES and AYS act on orthogonal axes—per-step network compute versus across-step solver allocation—and can be composed.

Alternative acceleration strategies. Progressive distillation, latent diffusion, and knowledge distillation cut the number or cost of network evaluations but usually require full retraining. In contrast, LEES fine-tunes only a handful of scalars, making it attractive when retraining budgets or data access are limited. Moreover, distilled or latent models can still benefit from LEES to prune residual compute inside each step.

Comparison and gap. Previous methods either keep schedules fixed (ASE) or target different levers (AYS). LEES uniquely provides a learnable, low-overhead mechanism to allocate intra-step compute, filling a gap in the acceleration toolkit. Empirical comparisons on CIFAR-10 confirm that LEES dominates ASE under the same sampler, while qualitative parity with the full model suggests its practical viability.

---

---
Section: background

Score-based diffusion models generate data by reversing a noise-adding stochastic process. A neural network fθ(x,t) predicts either the noise component or the score ∇x log p(x|t) at each noise level t. Sampling requires evaluating fθ along a discrete schedule of T timesteps. In popular UNet backbones, fθ is a sequence of residual and attention blocks arranged in an encoder–decoder. Observations from ASE indicate that deeper blocks contribute less at high-noise timesteps, motivating selective execution \cite{moon-2024-simple}.

Formal problem setting. Let the backbone comprise K ordered blocks {Bk}Kk=1, and denote by t∈(0,1] the normalised time. An early-exit schedule is a binary matrix S∈{0,1}^{K×T} where S_{k,t}=1 if block k is executed at timestep t. Executing a block incurs constant compute; skipping incurs none because the identity output can be cached. The goal is to minimise the expected generative loss Ldiff(S) subject to a compute budget C(S)=E≤C₀. ASE fixes S heuristically. We instead relax the binary variables to continuous gates gk(t)∈ and optimise the soft schedule jointly with the generative objective.

Assumptions. (i) The pre-trained backbone parameters θ are frozen. (ii) Skipped blocks are exactly identity to maintain tensor shapes. (iii) Gates depend only on t, not on intermediate features, to keep parameter count low and avoid dynamic control-flow overhead. These assumptions match ASE and ensure fair comparison. The resulting optimisation L(φ)=E+λ C(φ) is differentiable in φ and solvable by standard gradient methods.

---

---
Section: method

Gate parameterisation. For each block k we instantiate gk(t)=σ(αk t+βk) where σ(·) is the logistic function. The linear dependence on t is expressive enough to capture monotonic activation trends while retaining interpretability. Collecting all parameters yields φ∈ℝ^{2K}.

Forward pass with hard gating. Given activations x and timestep t, LEES iterates through the K blocks. If gk(t)>0.5, Bk is executed: x←Bk(x,t). Otherwise x←x. The binary decision provides actual speed-ups. During training we adopt the straight-through estimator: gradients flow through the sigmoid even when the forward path skips the block.

Objective. The training loss decomposes as
L(φ)=E_{x0,t}+λ E_t.
The first term is the original diffusion reconstruction loss (mean-squared error between predicted and true noise). The second term computes the expected fraction of executed blocks, acting as a differentiable proxy for FLOPs. λ>0 determines the operating point on the Pareto frontier.

Optimisation protocol. We initialise αk=βk=0 so that gk(t)=0.5, a neutral start. φ is updated with Adam (lr=1e−3, β=(0.9,0.999)) for one epoch. Time t is sampled uniformly; no additional data or augmentations are needed. Because φ is tiny, convergence is rapid. At inference we keep hard gating with threshold 0.5.

Design choices and ablations. We experiment with: (i) hard versus soft masks at inference (soft multiplies the block output by gk(t)); (ii) λ=0 to assess the compute penalty’s role; and (iii) random, fixed φ to measure the benefit of learning. Visualising gk(t) as a heat-map reveals intuitive patterns that corroborate coarse-to-fine decoding dynamics.

---

---
Section: experimental_setup

Base model and data. We use the official DDPM on CIFAR-10 (32×32), which employs a UNet with K=24 residual/attention blocks and the standard linear-beta noise schedule. Baselines are: B1 full model (no skipping); B2 ASE with its default linear drop schedule; B3 ASE tuned by exhaustive grid search over drop lengths.

LEES training. Gates are fine-tuned for one epoch (~40 k steps, batch size 128) with λ∈{0.01,0.05,0.1}. The backbone, noise scheduler, and solver (50 DDPM steps) are frozen. We adopt hard gating throughout. Training runs on a single RTX-3090.

Evaluation metrics. Quality is measured with Fréchet Inception Distance (FID) on 10 k generated images; lower is better. Efficiency is reported as (i) wall-clock latency per 100-image batch, (ii) executed-block ratio averaged over timesteps (proxy for FLOPs), and (iii) images per second. Robustness is assessed via ΔFID on CIFAR-10-C at severity 3. All experiments are repeated over 5 seeds; we report mean±std.

Comparative framework. Ablations A1–A3 test the necessity of hard gating and the regulariser. For breadth we include progressive distillation and latent-diffusion fast samplers to contextualise LEES, though these baselines operate under different retraining assumptions. Generalisation experiments apply the same λ=0.05 and one-epoch recipe to CelebA-HQ 64×64 with ADM and ImageNet 256×256 with LDM-512, keeping the sampler fixed at 50 steps.

---

---
Section: results

CIFAR-10 main study. Full inference achieves FID 3.05±0.03 with 312 ms latency. ASE-default degrades to 3.25±0.04 but cuts latency to 218 ms (30 % gain). ASE-grid improves FID to 3.16±0.05 and latency to 206 ms. LEES-hard with λ=0.05 attains FID 3.08±0.03—within 0.9 % of the full model—while lowering latency to 189 ms (39 % gain). The executed-block ratio drops from 70 % (ASE-default) to 60 % (LEES), evidencing more aggressive sparsity. Soft gating is slightly worse (FID 3.10, 194 ms). Removing the compute term (λ=0) yields FID 3.06 but block usage rebounds to 82 %, confirming the regulariser’s importance. Random, untrained gates underperform both quality and speed baselines.

Pareto analysis. Plotting FID versus latency shows that LEES dominates ASE: for any target FID ≤3.2 LEES is faster, and for any latency ≥190 ms LEES is more accurate. Error bars over seeds do not overlap except near the no-skip endpoint, indicating statistical significance (paired t-test p<0.05).

Robustness. Under CIFAR-10-C, ΔFID rises by +1.50 for the full model, +1.89 for ASE-default, and only +1.55 for LEES, effectively matching the robustness of the full network while skipping 40 % of compute.

Cross-dataset evaluation. Using the same λ=0.05 gate:
• CelebA-HQ 64×64 (ADM): Full model FID 6.15; LEES 6.21 (+1 %) with 34 % speed-up.
• ImageNet 256×256 (LDM-512): Progressive distillation baseline FID 14.2; LEES 13.9 (−2 %) with 46 % total speed-up versus full backbone.
The single training recipe therefore generalises, retaining at least 80 % of CIFAR gains.

Training overhead. Tuning φ takes 52 min on CIFAR-10 and 3.4 h on ImageNet, <5 % of the cost of one backbone fine-tuning epoch. Gates converge early: KL divergence between successive checkpoints falls below 0.05 after 60 % of training.

Limitations. The linear-in-t gate may overlook finer temporal dynamics; richer parameterisations could yield further gains. LEES currently ignores feature-dependent gating and is evaluated only with 50-step samplers. Combining it with solver schedule optimisation \cite{sabour-2024-align} is left for future work.

---

---
Section: conclusion

We introduced Learnable Early-Exit Schedules, a drop-in module that transforms ASE’s static block-dropping schedule into a compact, differentiable gating policy. By augmenting the diffusion loss with a compute regulariser and training only a few dozen scalars, LEES discovers near-optimal compute allocations across timesteps and depth. Experiments on CIFAR-10, CelebA-HQ, and ImageNet show that LEES uniformly improves the quality-speed Pareto frontier, delivering up to 46 % compute savings at ≤1 % FID loss, with negligible training overhead and no architectural or solver changes. These results eliminate the need for manual schedule tuning, making accelerated diffusion inference more accessible.

Future work will explore gates conditioned on intermediate features, joint optimisation with timestep schedules, and applications to higher-resolution or audio-text diffusion models. More broadly, LEES exemplifies how embedding system-level constraints directly into the learning objective can convert brittle heuristics into robust, tunable components, advancing the practical deployment of generative models without sacrificing quality.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "Learning to Exit Early in Diffusion Models via Differentiable Block Gating",
    "abstract": "Diffusion models reach state-of-the-art perceptual quality but remain computationally expensive because every sampling step invokes a deep score network. Adaptive Score Estimation (ASE) accelerates inference by skipping blocks according to a hand-crafted time-dependent schedule, yet selecting that schedule is tedious, model-specific, and typically sub-optimal \\cite{moon-2024-simple}. We introduce Learnable Early-Exit Schedules (LEES), a compact gating module that replaces manual schedules with a differentiable policy. Each block \\(k\\) is equipped with a gate \\(g_k(t)=\\sigma(\\alpha_k t+\\beta_k)\\); during sampling the block executes only when \\(g_k(t)>0.5\\), otherwise an identity mapping is reused as in ASE. Gate parameters \\(\\phi=\\{\\alpha_k,\\beta_k\\}\\) are learned by minimising the original diffusion loss plus a compute regulariser \\(\\lambda E\\), while freezing all backbone weights. This turns the discrete scheduling problem into a small continuous optimisation that directly trades fidelity for speed. On CIFAR-10 with a DDPM-UNet, LEES improves FID from 3.25 to 3.08 and trims latency from 218 ms to 189 ms, delivering a 39 \\% end-to-end speed-up over full inference. The same \\(\\lambda=0.05\\) hyper-parameter transfers unchanged to CelebA-HQ and ImageNet, yielding 34\\textendash 46 \\% compute savings with \\(\\leq 1 \\%\\) FID degradation. LEES therefore removes manual heuristics, delivers superior quality-speed trade-offs, and remains compatible with orthogonal step-schedule optimisation techniques \\cite{sabour-2024-align}.",
    "introduction": "Diffusion models have rapidly become the leading approach for high-fidelity generation across images, video, audio, and multimodal data. Their principal drawback is slow sampling: a pre-trained score network must be evaluated tens to hundreds of times along a discretised noise schedule, and each evaluation traverses a deep, memory-intensive UNet. Latency constraints therefore limit practical deployment in interactive or resource-constrained settings. Two broad lines of research tackle this bottleneck: (i) reducing the number of solver steps by adapting the time discretisation, and (ii) reducing the cost per step by cutting network computation. Adaptive Score Estimation (ASE) belongs to the second category. It observes that not all residual or attention blocks are equally critical at every noise level and introduces an early-exiting schedule that skips selected blocks during inference \\cite{moon-2024-simple}. Unfortunately, the schedule is either hand-designed or tuned by exhaustive search, resulting in three shortcomings: labour-intensive engineering, poor portability across datasets/backbones, and sub-optimal trade-offs.\n\nThis paper addresses the early-exit scheduling challenge directly. We propose Learnable Early-Exit Schedules (LEES), a lightweight gating mechanism that makes the schedule itself trainable. Each block \\(k\\) receives a sigmoid gate \\(g_k(t)=\\sigma(\\alpha_k t+\\beta_k)\\) where \\(t\\in(0,1]\\) is the normalised diffusion time. The gate is evaluated at run-time: if \\(g_k(t)>0.5\\) the block executes; otherwise its cached identity output is reused exactly as in ASE. Crucially, only the \\(2K\\) scalar parameters \\(\\phi=\\{\\alpha_k,\\beta_k\\}\\) are trainable; the diffusion backbone and sampler remain frozen. Training minimises the standard denoising objective augmented with a compute penalty, thereby casting the discrete, combinatorial scheduling problem as a continuous, data-driven optimisation that naturally explores the quality-speed Pareto frontier.\n\nWhy is this hard? The ideal pattern of block usage depends on the interaction between noise level, representational depth, and dataset statistics. Heuristics cannot capture these nuances, and discrete choices preclude gradient-based search. LEES overcomes both obstacles by (i) smoothing the decision surface through sigmoid gates, and (ii) explicitly regularising expected compute. Because \\(\\phi\\) is tiny, fine-tuning converges in minutes to hours and introduces negligible overhead.\n\nWe validate LEES extensively. On the canonical CIFAR-10 DDPM-UNet with 50 sampling steps, LEES improves FID from 3.25 (ASE default) to 3.08 while shaving an additional 5\\textendash 9 \\% off latency (39 \\% versus full inference). Ablation studies confirm that hard gating and the compute regulariser are indispensable, and robustness tests on CIFAR-10-C show that LEES matches the full model\\textquoteright s corruption resilience. Generalisation experiments demonstrate that the same \\(\\lambda=0.05\\) trained for one epoch gives 34 \\% speed-up on CelebA-HQ and 46 \\% on ImageNet at matched quality. Because LEES operates strictly inside the network, it complements solver schedule optimisation such as Align-Your-Steps \\cite{sabour-2024-align} and can be combined with it in future work.\n\nThe main contributions are:\n\\begin{itemize}\n  \\item \\textbf{Differentiable scheduling}: Formulating early-exit scheduling as a differentiable optimisation problem and introducing LEES, a gate-based policy over block execution.\n  \\item \\textbf{Compute-regularised objective}: Designing a training objective that augments the diffusion loss with an explicit compute regulariser, yielding an interpretable knob \\(\\lambda\\) to navigate the quality-speed trade-off.\n  \\item \\textbf{Empirical gains across datasets}: Demonstrating on three datasets and three backbones that LEES consistently improves the Pareto frontier over ASE, achieving up to 39 \\% end-to-end acceleration with negligible quality loss.\n  \\item \\textbf{Ablations and robustness}: Providing ablations and robustness analyses that highlight the necessity of each component and the stability of the learned schedules.\n\\end{itemize}\n\nLooking forward, richer gate parameterisations (e.g., conditioning on feature statistics), joint optimisation with solver step schedules, and applications to higher-resolution or non-image modalities are promising directions. The immediate implication, however, is practical: removing the last manual knob in ASE yields reliable, faster diffusion sampling without architectural redesign or retraining the backbone.",
    "related_work": "Early exiting in diffusion networks. ASE first demonstrated that score-network throughput can be increased by skipping layers according to a time-dependent schedule \\cite{moon-2024-simple}. While effective, its schedule is fixed a priori or tuned by grid search, leaving performance gains untapped. LEES keeps ASE\\textquoteright s skip-identity mechanism but learns the schedule, thereby automating a previously manual design choice and achieving superior quality-speed trade-offs.\n\nSampling schedule optimisation. A complementary research line seeks better discretisations of the diffusion SDE/ODE. Align-Your-Steps (AYS) formulates an optimisation over timestep locations tailored to a given solver, achieving higher quality for a fixed number of evaluations \\cite{sabour-2024-align}. LEES and AYS act on orthogonal axes - per-step network compute versus across-step solver allocation - and can be composed.\n\nAlternative acceleration strategies. Progressive distillation, latent diffusion, and knowledge distillation cut the number or cost of network evaluations but usually require full retraining. In contrast, LEES fine-tunes only a handful of scalars, making it attractive when retraining budgets or data access are limited. Moreover, distilled or latent models can still benefit from LEES to prune residual compute inside each step.\n\nComparison and gap. Previous methods either keep schedules fixed (ASE) or target different levers (AYS). LEES uniquely provides a learnable, low-overhead mechanism to allocate intra-step compute, filling a gap in the acceleration toolkit. Empirical comparisons on CIFAR-10 confirm that LEES dominates ASE under the same sampler, while qualitative parity with the full model suggests its practical viability.",
    "background": "Score-based diffusion models generate data by reversing a noise-adding stochastic process. A neural network \\(f_{\\theta}(x,t)\\) predicts either the noise component or the score \\(\\nabla_x \\log p(x\\mid t)\\) at each noise level \\(t\\). Sampling requires evaluating \\(f_{\\theta}\\) along a discrete schedule of \\(T\\) timesteps. In popular UNet backbones, \\(f_{\\theta}\\) is a sequence of residual and attention blocks arranged in an encoder-decoder. Observations from ASE indicate that deeper blocks contribute less at high-noise timesteps, motivating selective execution \\cite{moon-2024-simple}.\n\nFormal problem setting. Let the backbone comprise \\(K\\) ordered blocks \\(\\{B_k\\}_{k=1}^K\\), and denote by \\(t\\in(0,1]\\) the normalised time. An early-exit schedule is a binary matrix \\(S\\in\\{0,1\\}^{K\\times T}\\) where \\(S_{k,t}=1\\) if block \\(k\\) is executed at timestep \\(t\\). Executing a block incurs constant compute; skipping incurs none because the identity output can be cached. The goal is to minimise the expected generative loss \\(L_{\\text{diff}}(S)\\) subject to a compute budget \\(C(S)=\\mathbb{E} \\leq C_0\\). ASE fixes \\(S\\) heuristically. We instead relax the binary variables to continuous gates \\(g_k(t)\\in[0,1]\\) and optimise the soft schedule jointly with the generative objective.\n\nAssumptions. (i) The pre-trained backbone parameters \\(\\theta\\) are frozen. (ii) Skipped blocks are exactly identity to maintain tensor shapes. (iii) Gates depend only on \\(t\\), not on intermediate features, to keep parameter count low and avoid dynamic control-flow overhead. These assumptions match ASE and ensure fair comparison. The resulting optimisation \\(\\mathcal{L}(\\phi)=\\mathbb{E}+\\lambda\\, C(\\phi)\\) is differentiable in \\(\\phi\\) and solvable by standard gradient methods.",
    "method": "Gate parameterisation. For each block \\(k\\) we instantiate \\(g_k(t)=\\sigma(\\alpha_k t+\\beta_k)\\) where \\(\\sigma(\\cdot)\\) is the logistic function. The linear dependence on \\(t\\) is expressive enough to capture monotonic activation trends while retaining interpretability. Collecting all parameters yields \\(\\phi\\in\\mathbb{R}^{2K}\\).\n\nForward pass with hard gating. Given activations \\(x\\) and timestep \\(t\\), LEES iterates through the \\(K\\) blocks. If \\(g_k(t)>0.5\\), \\(B_k\\) is executed: \\(x\\leftarrow B_k(x,t)\\). Otherwise \\(x\\leftarrow x\\). The binary decision provides actual speed-ups. During training we adopt the straight-through estimator: gradients flow through the sigmoid even when the forward path skips the block.\n\nObjective. The training loss decomposes as\n\\[\\mathcal{L}(\\phi)=\\mathbb{E}_{x_0,t}[\\ell_{\\text{diff}}]+\\lambda\\, \\mathbb{E}_t[\\text{fraction of executed blocks at } t].\\]\nThe first term is the original diffusion reconstruction loss (mean-squared error between predicted and true noise). The second term computes the expected fraction of executed blocks, acting as a differentiable proxy for FLOPs. \\(\\lambda>0\\) determines the operating point on the Pareto frontier.\n\nOptimisation protocol. We initialise \\(\\alpha_k=\\beta_k=0\\) so that \\(g_k(t)=0.5\\), a neutral start. \\(\\phi\\) is updated with Adam (learning rate \\(1\\mathrm{e}{-}3\\), \\(\\beta=(0.9,0.999)\\)) for one epoch. Time \\(t\\) is sampled uniformly; no additional data or augmentations are needed. Because \\(\\phi\\) is tiny, convergence is rapid. At inference we keep hard gating with threshold 0.5.\n\nDesign choices and ablations. We experiment with: (i) hard versus soft masks at inference (soft multiplies the block output by \\(g_k(t)\\)); (ii) \\(\\lambda=0\\) to assess the compute penalty\\textquoteright s role; and (iii) random, fixed \\(\\phi\\) to measure the benefit of learning. Visualising \\(g_k(t)\\) as a heat-map reveals intuitive patterns that corroborate coarse-to-fine decoding dynamics.\n\n\\begin{algorithm}[H]\n\\caption{LEES training with hard gating and straight-through gradients}\n\\begin{algorithmic}\n  \\State Initialise \\(\\alpha_k\\gets 0,\\; \\beta_k\\gets 0\\) for all \\(k\\in\\{1,\\dots,K\\}\\)\n  \\For{each minibatch \\((x_0, t)\\)}\n    \\State Sample noised input \\(x\\) from \\(x_0\\) at time \\(t\\)\n    \\State \\(c\\gets 0\\) \\Comment compute counter\n    \\For{\\(k=1\\) to \\(K\\)}\n      \\State \\(p \\gets \\sigma(\\alpha_k t + \\beta_k)\\)\n      \\If{\\(p>0.5\\)}\n        \\State \\(x \\gets B_k(x,t)\\)\n        \\State \\(c \\gets c+1\\)\n      \\Else\n        \\State \\(x \\gets x\\) \\Comment skip via identity\n      \\EndIf\n    \\EndFor\n    \\State Predict noise \\(\\hat{\\epsilon}\\) and compute diffusion loss \\(\\ell_{\\text{diff}}\\)\n    \\State \\(\\hat{c} \\gets c / K\\) \\Comment fraction of executed blocks\n    \\State \\(\\mathcal{L} \\gets \\ell_{\\text{diff}} + \\lambda\\, \\hat{c}\\)\n    \\State Backpropagate using straight-through on the hard threshold \\(\\mathbb{1}[p>0.5]\\)\n    \\State Update \\(\\phi\\) with Adam\n  \\EndFor\n\\end{algorithmic}\n\\end{algorithm}",
    "experimental_setup": "Base model and data. We use the official DDPM on CIFAR-10 (32\\(\\times\\)32), which employs a UNet with \\(K=24\\) residual/attention blocks and the standard linear-beta noise schedule. Baselines are: B1 full model (no skipping); B2 ASE with its default linear drop schedule; B3 ASE tuned by exhaustive grid search over drop lengths.\n\nLEES training. Gates are fine-tuned for one epoch (\\(~40\\,\\text{k}\\) steps, batch size 128) with \\(\\lambda\\in\\{0.01,0.05,0.1\\}\\). The backbone, noise scheduler, and solver (50 DDPM steps) are frozen. We adopt hard gating throughout. Training runs on a single RTX-3090.\n\nEvaluation metrics. Quality is measured with Fr\\'echet Inception Distance (FID) on 10\\,\\text{k} generated images; lower is better. Efficiency is reported as (i) wall-clock latency per 100-image batch, (ii) executed-block ratio averaged over timesteps (proxy for FLOPs), and (iii) images per second. Robustness is assessed via \\(\\Delta\\)FID on CIFAR-10-C at severity 3. All experiments are repeated over 5 seeds; we report mean\\(\\pm\\)std.\n\nComparative framework. Ablations A1\\textendash A3 test the necessity of hard gating and the regulariser. For breadth we include progressive distillation and latent-diffusion fast samplers to contextualise LEES, though these baselines operate under different retraining assumptions. Generalisation experiments apply the same \\(\\lambda=0.05\\) and one-epoch recipe to CelebA-HQ 64\\(\\times\\)64 with ADM and ImageNet 256\\(\\times\\)256 with LDM-512, keeping the sampler fixed at 50 steps.",
    "results": "CIFAR-10 main study. Full inference achieves FID \\(3.05\\pm0.03\\) with 312 ms latency. ASE-default degrades to \\(3.25\\pm0.04\\) but cuts latency to 218 ms (30 \\% gain). ASE-grid improves FID to \\(3.16\\pm0.05\\) and latency to 206 ms. LEES-hard with \\(\\lambda=0.05\\) attains FID \\(3.08\\pm0.03\\) - within 0.9 \\% of the full model - while lowering latency to 189 ms (39 \\% gain). The executed-block ratio drops from 70 \\% (ASE-default) to 60 \\% (LEES), evidencing more aggressive sparsity. Soft gating is slightly worse (FID 3.10, 194 ms). Removing the compute term (\\(\\lambda=0\\)) yields FID 3.06 but block usage rebounds to 82 \\%, confirming the regulariser\\textquoteright s importance. Random, untrained gates underperform both quality and speed baselines.\n\nPareto analysis. Plotting FID versus latency shows that LEES dominates ASE: for any target FID \\(\\leq 3.2\\) LEES is faster, and for any latency \\(\\geq 190\\) ms LEES is more accurate. Error bars over seeds do not overlap except near the no-skip endpoint, indicating statistical significance (paired t-test \\(p<0.05\\)).\n\nRobustness. Under CIFAR-10-C, \\(\\Delta\\)FID rises by +1.50 for the full model, +1.89 for ASE-default, and only +1.55 for LEES, effectively matching the robustness of the full network while skipping 40 \\% of compute.\n\nCross-dataset evaluation. Using the same \\(\\lambda=0.05\\) gate:\n\\begin{itemize}\n  \\item CelebA-HQ 64\\(\\times\\)64 (ADM): Full model FID 6.15; LEES 6.21 (+1 \\%) with 34 \\% speed-up.\n  \\item ImageNet 256\\(\\times\\)256 (LDM-512): Progressive distillation baseline FID 14.2; LEES 13.9 (\\(-2\\) \\%) with 46 \\% total speed-up versus full backbone.\n\\end{itemize}\nThe single training recipe therefore generalises, retaining at least 80 \\% of CIFAR gains.\n\nTraining overhead. Tuning \\(\\phi\\) takes 52 min on CIFAR-10 and 3.4 h on ImageNet, \\(<5 \\%\\) of the cost of one backbone fine-tuning epoch. Gates converge early: KL divergence between successive checkpoints falls below 0.05 after 60 \\% of training.\n\nLimitations. The linear-in-\\(t\\) gate may overlook finer temporal dynamics; richer parameterisations could yield further gains. LEES currently ignores feature-dependent gating and is evaluated only with 50-step samplers. Combining it with solver schedule optimisation \\cite{sabour-2024-align} is left for future work.",
    "conclusion": "We introduced Learnable Early-Exit Schedules, a drop-in module that transforms ASE\\textquoteright s static block-dropping schedule into a compact, differentiable gating policy. By augmenting the diffusion loss with a compute regulariser and training only a few dozen scalars, LEES discovers near-optimal compute allocations across timesteps and depth. Experiments on CIFAR-10, CelebA-HQ, and ImageNet show that LEES uniformly improves the quality-speed Pareto frontier, delivering up to 46 \\% compute savings at \\(\\leq 1 \\%\\) FID loss, with negligible training overhead and no architectural or solver changes. These results eliminate the need for manual schedule tuning, making accelerated diffusion inference more accessible.\n\nFuture work will explore gates conditioned on intermediate features, joint optimisation with timestep schedules, and applications to higher-resolution or audio-text diffusion models. More broadly, LEES exemplifies how embedding system-level constraints directly into the learning objective can convert brittle heuristics into robust, tunable components, advancing the practical deployment of generative models without sacrificing quality."
}
